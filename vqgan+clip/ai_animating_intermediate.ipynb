{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RichardSlater/ai-ml-playground/blob/main/vqgan%2Bclip/ai_animating_intermediate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMzL5Abrv3Wm"
      },
      "source": [
        "# Making Animations with cutting edge Artificial Inteligence\n",
        "\n",
        "This notebook uses VQGAN+CLIP to generate the frames and a series of animation techniques on top of it to zoom, rotate, and shift by x (left/right), y (up/down) pixels.  Optionally you can also use SRCNN to upscale the resulting images and Super-SloMo to create additional interpolated frames to create smoother motion.\n",
        "\n",
        "## Technology\n",
        "\n",
        "### Core Neural Networks\n",
        "\n",
        "#### Vector Quantized Generative Adversarial Network (VQGAN)\n",
        "\n",
        "Vector Quantized Generative Adversarial Network (\"VQGAN\") is a type of Generative Adveserial Network which pitches two neural networks against eachother in a \"Generate and Review\" relationship.  VQGAN is capable of taking text input and synthesising a picture or scene.\n",
        "\n",
        "Read more in [Taming Transformers for High-resolution Image Synthesis](https://compvis.github.io/taming-transformers/).\n",
        "\n",
        "#### Contrastive Language–Image Pre-training (CLIP)\n",
        "\n",
        "Contrastive Language–Image Pre-training is a neural network which compares an image with a caption and decides how well that image matches the caption.  In this context CLIP is used to assess the images produced by the other Neural Network.\n",
        "\n",
        "Read more in [CLIP: Connecting Text and Images](https://openai.com/blog/clip/).\n",
        "\n",
        "### Additional Neural Networks\n",
        "\n",
        "The following two neural networks are optional, you can happily create a video without them however by interpolating pixels and frames we can create higher resolutions and better quality videos.\n",
        "\n",
        "#### Super-Resolution Convolutional Neural Network (SRCNN)\n",
        "\n",
        "SRCNN is a convolutional neural network, in the sense that it uses the mathmatical concept of convolution to infer information about a subject.  SRCNN typically performs better than \"normal\" bicubic interpolation resulting in clearer images.\n",
        "\n",
        "Read more in [Original paper on SRCNN by Dong et al. (Image Super-Resolution Using Deep Convolutional Networks)](https://github.com/Mirwaisse/SRCNN).\n",
        "\n",
        "#### Super-SloMo\n",
        "\n",
        "Super-SlowMo is another convolutional neural network that looks at two frames and creates intermediate frames allowing for a lower overall frame rate whilst maintining the quality.  As it takes between 2-3 minutes to create 100 iterations of a frame on a NVidia P100 it would be prohibitivly timeconsuming to use VQGAN+CLIP to create a 30fps video, therefore using Super-SlowMo helps close the gap.\n",
        "\n",
        "Read more in \"[Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation](https://arxiv.org/abs/1712.00080)\" by Jiang H., Sun D., Jampani V., Yang M., Learned-Miller E. and Kautz J."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wguhQ-XdIgmf"
      },
      "source": [
        "## Credit\n",
        "\n",
        "Notebook by [Katherine Crowson](https://github.com/crowsonkb) [[Twitter](https://twitter.com/RiversHaveWings)]. Zoom, pan, rotation, and keyframes features by [Chigozie Nri](https://github.com/chigozienri) [[Twitter](https://twitter.com/chigozienri)]. Adapted by the A. I. Whisperer and [Richard Slater](https://github.com/RichardSlater/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FSv_7n-dy89"
      },
      "source": [
        "```\n",
        "# Licensed under the MIT License\n",
        "\n",
        "# Copyright (c) 2021 Katherine Crowson\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prepare Filesystem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "81kTCqx-sE8R"
      },
      "outputs": [],
      "source": [
        "#@title # Connect to Google Drive\n",
        "#@markdown Use Google Drive to provide persistent storage, Google Colabotory's\n",
        "#@markdown `/content` is ephemeral and is lost when the runtime disconnects.\n",
        "#@markdown *Note:* Google Colab VMs are small, and network IO is slow.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "xOcyvzypXc7T"
      },
      "outputs": [],
      "source": [
        "#@title # Processing options\n",
        "\n",
        "upscale_frames = True #@param {type:\"boolean\"}\n",
        "crop_frames = True #@param {type:\"boolean\"}\n",
        "generate_videos = True #@param {type:\"boolean\"}\n",
        "super_slomo = False #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oA9isL44gb5V"
      },
      "outputs": [],
      "source": [
        "#@title # Set filesystem variables\n",
        "#@markdown | Variable | Usage |\n",
        "#@markdown |---|---|\n",
        "#@markdown | `working_dir` | Directory where Git repos, `/steps`, `video.mp4`, etc. are stored |\n",
        "#@markdown | `backup_dir`  | Persistent directory for project files, make sure you **connect to Google Drive Above** |\n",
        "#@markdown | `cache_dir`   | Persistent directory for caches of models to avoid repeatedly downloading them when the runtime is reset |\n",
        "#@markdown | `models_dir`  | Location to keep the models at runtime, should be in the `/content` directory on Colab |\n",
        "\n",
        "working_dir = '/content' #@param {type:\"string\"}\n",
        "project_dir = '/content/drive/MyDrive/vqgan+clip/projects/testing' #@param {type:\"string\"}\n",
        "cache_dir = '/content/drive/MyDrive/vqgan+clip/cache' #@param {type:\"string\"}\n",
        "models_dir = '/content/models' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4INwpxFKgvbo"
      },
      "source": [
        "# Configuration\n",
        "## Instructions for setting Animation parameters\n",
        "\n",
        "| Parameter  |  Usage |\n",
        "|---|---|\n",
        "| `key_frames` | Using keyframes allows you to change animation parameters over time |\n",
        "|  `text_prompts` |  Input to Neural Network to Generate an Frame based off of, you can seperate by \"\\|\" to get different prompts |\n",
        "| `width` | Width of the output, in pixels. This will be rounded down to a multiple of 16 |\n",
        "| `height` | Height of the output, in pixels. This will be rounded down to a multiple of 16 |\n",
        "| `trim_width` | Number of pixels to remove from the width |\n",
        "| `trim_height` | Number of pixels to remove from the height |\n",
        "| `vqgan_model` | Choice of model, must be downloaded in the download models cell |\n",
        "| `interval` | How often to display the frame in the notebook (doesn't affect the actual output) |\n",
        "| `initial_image` | Image to start with (relative path to file) |\n",
        "| `target_images` | Image prompts to target, separated by \"|\" (relative path to files) |\n",
        "| `seed` | Random seed, if set to a positive integer the run will be repeatable (get the same output for the same input each time, if set to -1 a random seed will be used. |\n",
        "| `init_frame` | Frame to start from, allows resumption of a crashed session by uploading a backup of the last frame and setting `init_frame` and `initial_image` to the last good frame |\n",
        "| `max_frames` | Number of frames for the animation |\n",
        "| `angle` | Angle in degrees to rotate clockwise between each frame |\n",
        "| `zoom` | Factor to zoom in each frame, 1 is no zoom, less than 1 is zoom out, more than 1 is zoom in (negative is uninteresting, just adds an extra 180 rotation beyond that in angle) |\n",
        "| `translation_x` | Number of pixels to shift right each frame |\n",
        "| `translation_y` | Number of pixels to shift down each frame |\n",
        "| `iterations_per_frame` | Number of times to run the VQGAN+CLIP method each frame |\n",
        "| `save_all_iterations` | Debugging, set False in normal operation |\n",
        "\n",
        "## Process\n",
        "\n",
        "On each frame, the network restarts, is fed a version of the output zoomed in by `zoom` as the initial image, rotated clockwise by `angle` degrees, translated horizontally by `translation_x` pixels, and translated vertically by `translation_y` pixels. Then it runs `iterations_per_frame` iterations of the VQGAN+CLIP method. 0 `iterations_per_frame` is supported, to help test out the transformations without changing the image.\n",
        "\n",
        "For `iterations_per_frame = 1` (recommended for more abstract effects), the resulting images will not have much to do with the prompts, but at least one prompt is still required.\n",
        "\n",
        "In normal use, only the last iteration of each frame will be saved, but for trouble-shooting you can set `save_all_iterations` to True, and every iteration of each frame will be saved.\n",
        "\n",
        "![](https://raw.githubusercontent.com/RichardSlater/ai-ml-playground/main/vqgan%2Bclip/assets/vqgan%2Bclip-flow-intermediate.png)\n",
        "\n",
        "### Resolutions\n",
        "\n",
        "#### Common resolutions\n",
        "\n",
        "These are common resolutions used for these platforms, in most cases they are on the low-resolution side and may need to be upscaled before the network accepts the video:\n",
        " - **TikTok resolution**: 340px by 570px\n",
        " - **YouTube (16:9) resolution**: 640px by 360px (360p) or 426px x 240px (240p)\n",
        " - **4:3 resolution**: 480px by 360px\n",
        " - **TikTok/Instagram (Square)**: 500px by 500px or 400px by 400px\n",
        "\n",
        "You will find that any height or width that is not divisible by 16 is subject to change, you can entirely avoid this by selecting resolutions that that are disvisible by 16:\n",
        " - 144px by 256px\n",
        " - 288px by 512px\n",
        " - 432px by 768px (too large for a 16GB GPU)\n",
        " - 576px by 1024px (too large for a 24GB GPU)\n",
        " - 720px by 1280px (too large for a 40GB GPU)\n",
        " - 864px by 1536px (too large for a 40GB GPU)\n",
        "\n",
        "Alternatively you can generate more pixels than are requried by 8px then crop the vestigal 8px off during post processing.\n",
        "\n",
        "### Key Frames\n",
        "\n",
        "If `key_frames` is set to True, you are able to change the parameters over the course of the run.\n",
        "To do this, put the parameters in in the following format:\n",
        "10:(0.5), 20: (1.0), 35: (-1.0)\n",
        "\n",
        "This means at frame 10, the value should be 0.5, at frame 20 the value should be 1.0, and at frame 35 the value should be -1.0. The value at each other frame will be linearly interpolated (that is, before frame 10, the value will be 0.5, between frame 10 and 20 the value will increase frame-by-frame from 0.5 to 1.0, between frame 20 and 35 the value will decrease frame-by-frame from 1.0 to -1.0, and after frame 35 the value will be -1.0)\n",
        "\n",
        "This also works for text_prompts, e.g. 10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\n",
        "will start with an Apple value of 1, once it hits frame 10 it will start decreasing in in Apple and increasing in Orange until it hits frame 20. Note that Peach will have a value of 1 the whole time.\n",
        "\n",
        "If `key_frames` is set to True, all of the parameters which can be key-framed must be entered in this format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4Qf6-IALXc7V"
      },
      "outputs": [],
      "source": [
        "#@title # Parameters for Model and Animating\n",
        "\n",
        "key_frames = True #@param {type:\"boolean\"}\n",
        "text_prompts = \"0:(shouting at the moon night time scene in unreal engine hyperrealistic: 1)\" #@param {type:\"string\"}\n",
        "width = 368 #@param {type:\"slider\", min:0, max:1536, step:1}\n",
        "height = 640 #@param {type:\"slider\", min:0, max:1536, step:1}\n",
        "trim_width = 8 #@param {type:\"slider\", min:0, max:1536, step:1}\n",
        "trim_height = 0 #@param {type:\"slider\", min:0, max:1536, step:1}\n",
        "interval = 1 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "initial_image = \"\" #@param {type:\"string\"}\n",
        "target_images = \"\" #@param {type:\"string\"}\n",
        "seed = 183264307 #@param {type:\"number\"}\n",
        "max_frames = 5 #@param {type:\"slider\", min:1, max:600, step:1}\n",
        "angle = \"0: (0)\" #@param {type:\"string\"}\n",
        "zoom = \"0: (1.05)\" #@param {type:\"string\"}\n",
        "translation_x = \"0: (0)\" #@param {type:\"string\"}\n",
        "translation_y = \"0: (0)\" #@param {type:\"string\"}\n",
        "iterations_per_frame = \"0: (3)\" #@param {type:\"string\"}\n",
        "save_all_iterations = False #@param {type:\"boolean\"}\n",
        "superscale_resolution = \"3x\" #@param [\"2x\", \"3x\", \"4x\"] {type:\"string\"}\n",
        "target_fps = 12 #@param {type:\"slider\", min:0, max:30, step:1}\n",
        "\n",
        "init_frame = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1Mhw_VxLXc7V"
      },
      "outputs": [],
      "source": [
        "#@title # Configure Models\n",
        "#@markdown By default, the notebook downloads S-FLCKR. There are others such as ImageNet 16385, ImageNet 1024, COCO-Stuff, WikiArt 1024, WikiArt 16384 or FacesHQ, which are not downloaded by default, since it would be in vain if you are not going to use them, so if you want to use them, simply select the models to download.\n",
        "\n",
        "imagenet_1024 = False #@param {type:\"boolean\"}\n",
        "imagenet_16384 = False #@param {type:\"boolean\"}\n",
        "coco = False #@param {type:\"boolean\"}\n",
        "faceshq = False #@param {type:\"boolean\"}\n",
        "wikiart_16384 = False #@param {type:\"boolean\"}\n",
        "sflckr = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown this is the model that will actually be used \n",
        "vqgan_model = \"sflckr\" #@param [\"vqgan_imagenet_f16_16384\", \"vqgan_imagenet_f16_1024\", \"wikiart_16384\", \"coco\", \"faceshq\", \"sflckr\"]\n",
        "\n",
        "#@markdown Note that some datasets are not compatible with commercial use:\n",
        "#@markdown - imagenet_1024, imagenet_16384: research, non-commercial\n",
        "#@markdown - coco: [various](https://github.com/nightrome/cocostuff#licensing)\n",
        "#@markdown - faceshq: [CC-BY-NC-4.0](https://github.com/NVlabs/ffhq-dataset/blob/master/LICENSE.txt)\n",
        "#@markdown - wikiart_16384: non-commercial\n",
        "#@markdown - sflckr: public domain\n",
        "\n",
        "model_names={\n",
        "    \"vqgan_imagenet_f16_16384\": 'ImageNet 16384',\n",
        "    \"vqgan_imagenet_f16_1024\":\"ImageNet 1024\", \n",
        "    \"wikiart_1024\":\"WikiArt 1024\",\n",
        "    \"wikiart_16384\":\"WikiArt 16384\",\n",
        "    \"coco\":\"COCO-Stuff\",\n",
        "    \"faceshq\":\"FacesHQ\",\n",
        "    \"sflckr\":\"S-FLCKR\"\n",
        "}\n",
        "model_name = model_names[vqgan_model]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53M1ncKiXc7W"
      },
      "source": [
        "----\n",
        "\n",
        "You should not need to change anything below as all of the configuration has been moved above this line!\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ws1CVwGyXc7W"
      },
      "outputs": [],
      "source": [
        "#@title ## Setup Project\n",
        "#@markdown We create a `NeuralProject` class as a utility class to simplify session save and resumption.\n",
        "\n",
        "%pip install pyyaml\n",
        "\n",
        "from os.path import join, isfile, split\n",
        "from os import makedirs\n",
        "from shutil import copy\n",
        "import yaml\n",
        "\n",
        "\n",
        "def dump(obj):\n",
        "    for attr in dir(obj):\n",
        "        if attr.startswith('__'):\n",
        "            continue\n",
        "        print(\"obj.%s = %r\" % (attr, getattr(obj, attr)))\n",
        "\n",
        "\n",
        "class NeuralProject:\n",
        "    __project_file = None\n",
        "    __project_dir = None\n",
        "    __gpus = []\n",
        "    __frames = {}\n",
        "    __outputs = {}\n",
        "    __config = {}\n",
        "\n",
        "\n",
        "    def __init__(self, project_dir):\n",
        "        self.__project_dir = project_dir\n",
        "        self.__project_file = join(project_dir, 'project.yaml')\n",
        "        makedirs(project_dir, exist_ok = True)\n",
        "\n",
        "\n",
        "    def exists(self):\n",
        "        return isfile(self.__project_file)\n",
        "\n",
        "\n",
        "    def __get_stage_dir(self, stage):\n",
        "        return join(self.__project_dir, stage)\n",
        "\n",
        "\n",
        "    def add_gpu(self, gpu, uuid):\n",
        "        gpu_uuid = f'{gpu}: {uuid}'\n",
        "        \n",
        "        if len(uuid) == 0:\n",
        "            return\n",
        "\n",
        "        if self.__gpus.count(gpu_uuid) == 0:\n",
        "            self.__gpus.append(gpu_uuid)\n",
        "\n",
        "\n",
        "    def add_artifact(self, stage, frame, artifact_file):\n",
        "        self.__frames[stage] = frame\n",
        "\n",
        "        stage_dir = self.__get_stage_dir(stage)\n",
        "        makedirs(stage_dir, exist_ok=True)\n",
        "\n",
        "        artifact_filename = split(artifact_file)[1]\n",
        "        backup_file = join(stage_dir, artifact_filename) \n",
        "        copy(artifact_file, backup_file)\n",
        "\n",
        "        self.__save_project()\n",
        "\n",
        "        print(f'Stored frame {frame} from {stage} ({artifact_file} => {backup_file})')\n",
        "\n",
        "\n",
        "    def add_output(self, stage, output_file):\n",
        "        if not stage in self.__outputs:\n",
        "            self.__outputs[stage] = []\n",
        "        \n",
        "        self.__outputs[stage].append(output_file)\n",
        "        output_filename = split(output_file)[1]\n",
        "\n",
        "        stage_outputs_dir = join(self.__project_dir, 'outputs', stage)\n",
        "        makedirs(stage_outputs_dir, exist_ok=True)\n",
        "\n",
        "        backup_file = join(stage_outputs_dir, output_filename)\n",
        "        copy(output_file, backup_file)\n",
        "\n",
        "        self.__save_project()\n",
        "\n",
        "        print(f'Stored output from {stage} ({output_file} => {backup_file})')\n",
        "\n",
        "\n",
        "    def add_config(self, key, value):\n",
        "        self.__config[key] = value\n",
        "\n",
        "\n",
        "    def get_config(self, key):\n",
        "        return self.__config[key]\n",
        "\n",
        "\n",
        "    def check_config(self, key):\n",
        "        return key in self.__config.keys()\n",
        "\n",
        "\n",
        "    def __save_project(self):\n",
        "        data = dict(\n",
        "            gpus = self.__gpus,\n",
        "            frames = self.__frames,\n",
        "            outputs = self.__outputs,\n",
        "            config = self.__config\n",
        "        )\n",
        "        with open(self.__project_file, 'w') as outfile:\n",
        "            yaml.dump(data, outfile, default_flow_style=False)\n",
        "\n",
        "\n",
        "    def __load_project(self):\n",
        "        with open(self.__project_file, 'r') as infile:\n",
        "            data = yaml.safe_load(infile)\n",
        "\n",
        "            self.__config = data[\"config\"]\n",
        "            self.__gpus = data[\"gpus\"]\n",
        "            self.__frames = data[\"frames\"]\n",
        "            self.__outputs = data[\"outputs\"]\n",
        "\n",
        "        print(f\"Config: {self.__config}\")\n",
        "        print(f\"GPUs: {self.__gpus}\")\n",
        "        print(f\"Frames: {self.__frames}\")\n",
        "        print(f\"Outputs: {self.__outputs}\")\n",
        "\n",
        "\n",
        "    def __check_stage(self, stage):\n",
        "        last_frame = self.__frames[stage]\n",
        "        stage_dir = self.__get_stage_dir(stage)\n",
        "        dirty = False\n",
        "\n",
        "        for i in range(1, last_frame):\n",
        "            frame = join(stage_dir, f\"{i:04d}.png\")\n",
        "            if (not isfile(frame)):\n",
        "                print(f\"  {frame} missing\")\n",
        "                dirty = True\n",
        "                continue\n",
        "\n",
        "        if dirty:\n",
        "            print(\"At least one frame in the sequence is missing!\")\n",
        "        else:\n",
        "            print(f\"All frames appear to be in place.\")\n",
        "\n",
        "\n",
        "    def restore_project(self):\n",
        "        self.__load_project()\n",
        "\n",
        "        for stage in self.__frames.keys():\n",
        "            self.__check_stage(stage)\n",
        "            backup_dir = self.__get_stage_dir(stage)\n",
        "            restore_dir = join(working_dir, stage)\n",
        "            for target_file in listdir(backup_dir):\n",
        "                backup_file = join(backup_dir, target_file)\n",
        "                restore_file = join(restore_dir, target_file)\n",
        "                copy(backup_file, restore_file)\n",
        "\n",
        "\n",
        "project = NeuralProject(project_dir)\n",
        "\n",
        "notebook_config_vars = [\"upscale_frames\", \"crop_frames\", \"super_slomo\", \"generate_videos\", \"key_frames\", \"text_prompts\", \n",
        "    \"width\", \"height\", \"trim_width\", \"trim_height\", \"interval\", \"initial_image\", \"target_images\", \"seed\", \"max_frames\",\n",
        "    \"angle\", \"zoom\", \"translation_x\", \"translation_y\", \"iterations_per_frame\", \"save_all_iterations\", \"superscale_resolution\",\n",
        "    \"target_fps\", \"vqgan_model\"]\n",
        "\n",
        "if (project.exists()):\n",
        "    project.restore_project()\n",
        "    for config_var_name in notebook_config_vars:\n",
        "        if not project.check_config(config_var_name):\n",
        "            print(f\"The config variable '{config_var_name}' was not found in the project config.\")\n",
        "            continue\n",
        "\n",
        "        config_value = project.get_config(config_var_name)\n",
        "        if config_value != globals()[config_var_name]:\n",
        "            print(f\"WARNING: Loading {config_var_name} from file, overwriting the value '{globals()[config_var_name]}' with {config_value}\")\n",
        "            globals()[config_var_name] = config_value\n",
        "else:\n",
        "    for config_var_name in notebook_config_vars:\n",
        "        project.add_config(config_var_name, globals()[config_var_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kkD9si8Wwvvg"
      },
      "outputs": [],
      "source": [
        "#@title ## Check GPU type\n",
        "\n",
        "#@markdown Factory reset runtime if you don't have the desired GPU.\n",
        "#@markdown\n",
        "#@markdown | GPU  | Memory | Information |\n",
        "#@markdown |---   |    ---:|---          |\n",
        "#@markdown | P100 | 16GB   | Very good GPU, typically takes 2-3 minutes per frame of ~16k pixels |\n",
        "#@markdown | T4   | 16GB   | Good GPU, typically takes 3-4 minutes per frame of ~16k pixels |\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "gpu_detail = !nvidia-smi --query --xml-format\n",
        "\n",
        "root = ET.ElementTree(ET.fromstringlist(gpu_detail)).getroot()\n",
        "for child in root:\n",
        "    if child.tag == 'driver_version':\n",
        "        print(f'NVIDIA Driver Version: {child.text}')\n",
        "    elif child.tag == 'cuda_version':\n",
        "        print(f'CUDA Version: {child.text}')\n",
        "    elif child.tag == 'gpu':\n",
        "        gpu_product = ''\n",
        "        gpu_uuid = ''\n",
        "        print(f'{child.attrib[\"id\"]}:')\n",
        "        for gpu_property in child:\n",
        "            if gpu_property.tag == 'product_name':\n",
        "                print(f'  Product Name : {gpu_property.text}')\n",
        "                gpu_product = gpu_property.text\n",
        "            elif gpu_property.tag == 'product_brand':\n",
        "                print(f'  Brand        : {gpu_property.text}')\n",
        "            elif gpu_property.tag == 'product_architecture':\n",
        "                print(f'  Architecture : {gpu_property.text}')\n",
        "            elif gpu_property.tag == 'uuid':\n",
        "                print(f'  UUID         : {gpu_property.text}')\n",
        "                gpu_uuid = gpu_property.text\n",
        "            elif gpu_property.tag == 'fb_memory_usage':\n",
        "                for mem in gpu_property:\n",
        "                    if mem.tag == 'total':\n",
        "                        print(f'  Total Memory : {mem.text}')\n",
        "            project.add_gpu(gpu_product, gpu_uuid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ngVYOAZEghkK"
      },
      "outputs": [],
      "source": [
        "# @title ## Library installation\n",
        "# @markdown This cell will take a while because you have to download multiple libraries\n",
        "\n",
        "from os import makedirs\n",
        "from os.path import join\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "print(\"Downloading CLIP...\")\n",
        "!git clone https://github.com/openai/CLIP                                      &> /dev/null\n",
        " \n",
        "print(\"Downloading Taming Transformers (VQGAN)...\")\n",
        "!git clone https://github.com/CompVis/taming-transformers                      &> /dev/null\n",
        "\n",
        "print(\"Downloading Installing Python AI libraries...\")\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning                       &> /dev/null\n",
        "!pip install kornia                                                            &> /dev/null\n",
        "!pip install einops                                                            &> /dev/null\n",
        " \n",
        "print(\"Installing libraries for handling metadata...\")\n",
        "!pip install stegano                                                           &> /dev/null\n",
        "!apt install exempi                                                            &> /dev/null\n",
        "!pip install python-xmp-toolkit                                                &> /dev/null\n",
        "!pip install imgtag                                                            &> /dev/null\n",
        "!pip install pillow==7.1.2                                                     &> /dev/null\n",
        " \n",
        "print(\"Installing Python video creation libraries...\")\n",
        "!pip install imageio-ffmpeg                                                    &> /dev/null\n",
        "!pip install ffmpeg-python                                                     &> /dev/null\n",
        "\n",
        "print(\"Downloading SRCNN\")\n",
        "!git clone https://github.com/Mirwaisse/SRCNN.git                              &> /dev/null\n",
        "\n",
        "print(\"Downloading Super-SlowMo\")\n",
        "!git clone -q --depth 1 https://github.com/avinashpaliwal/Super-SloMo.git      &> /dev/null\n",
        "\n",
        "path = join(working_dir, 'steps')\n",
        "makedirs(path, exist_ok=True)\n",
        "\n",
        "print(\"Installation finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5WOrrsX3Xc7Y"
      },
      "outputs": [],
      "source": [
        "# @title ## Install caching framework\n",
        "\n",
        "from os.path import isfile, abspath, join\n",
        "from os import makedirs\n",
        "from urllib.request import urlretrieve\n",
        "from shutil import copy\n",
        "\n",
        "if len(cache_dir.strip()) == 0:\n",
        "    raise RuntimeError(\"cache_dir is not set, please set it in variables above.\")\n",
        "\n",
        "if len(models_dir.strip()) == 0:\n",
        "    raise RuntimeError(\"models_dir is not set, please set it in variables above.\")\n",
        "\n",
        "model_cache_dir = abspath(join(cache_dir, 'models')) # models cache should in general be in a persistent disk\n",
        "\n",
        "# create directories if they don't exist.\n",
        "makedirs(models_dir, exist_ok=True)\n",
        "makedirs(cache_dir, exist_ok=True)\n",
        "makedirs(model_cache_dir, exist_ok=True)\n",
        "\n",
        "def download_vqgan_model(name, config_url, checkpoint_url):\n",
        "    # create a directory to store the models in\n",
        "    model_cache = join (model_cache_dir, name)\n",
        "    makedirs(model_cache, exist_ok=True)\n",
        "\n",
        "    # create vairables for the config\n",
        "    config_filename = f'{name}.yaml'\n",
        "    config_file = join(models_dir, config_filename)\n",
        "    config_cache = join(model_cache, config_filename)\n",
        "\n",
        "    # create variables for the checkpoint\n",
        "    checkpoint_filename = f'{name}.ckpt'\n",
        "    checkpoint_file = join(models_dir, checkpoint_filename)\n",
        "    checkpoint_cache = join(model_cache, checkpoint_filename)\n",
        "\n",
        "    # test for availability of the files\n",
        "    is_available = isfile(config_file) and isfile(checkpoint_file)\n",
        "    is_cached = isfile(config_cache) and isfile(checkpoint_cache)\n",
        "\n",
        "    # just return the name of the model it's already available locally\n",
        "    if (is_available):\n",
        "        print(f'The model ({name}) is already available locally.')\n",
        "        return name\n",
        "\n",
        "    # copy the files from the cache and return the name of the model\n",
        "    if (is_cached):\n",
        "        print(f'The model ({name}) is available in the cache, copying locally.')\n",
        "        copy(config_cache, config_file)\n",
        "        copy(checkpoint_cache, checkpoint_file)\n",
        "        return name\n",
        "\n",
        "    # downlaod everything, cache it and return the name of the model\n",
        "    print(f'The model ({name}) was not found, downloading and caching.')\n",
        "    urlretrieve(config_url, config_file)\n",
        "    urlretrieve(checkpoint_url, checkpoint_file)\n",
        "    copy(config_file, config_cache)\n",
        "    copy(checkpoint_file, checkpoint_cache)\n",
        "    return name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xkgbrCZqgkFw"
      },
      "outputs": [],
      "source": [
        "# @title ## Download required models\n",
        "\n",
        "models = []\n",
        "\n",
        "if imagenet_1024:\n",
        "    downloded_model = download_vqgan_model(\n",
        "        name='vqgan_imagenet_f16_1024',\n",
        "        config_url='https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1',\n",
        "        checkpoint_url='https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fckpts%2Flast.ckpt&dl=1')\n",
        "    models.append(downloded_model)\n",
        "if imagenet_16384:\n",
        "    downloded_model = download_vqgan_model(\n",
        "        name='vqgan_imagenet_f16_16384',\n",
        "        config_url='https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1',\n",
        "        checkpoint_url='https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1')\n",
        "    models.append(downloded_model)\n",
        "if coco:\n",
        "    downloded_model = download_vqgan_model(\n",
        "        name='coco',\n",
        "        config_url='https://dl.nmkd.de/ai/clip/coco/coco.yaml',\n",
        "        checkpoint_url='https://dl.nmkd.de/ai/clip/coco/coco.ckpt')\n",
        "    models.append(downloded_model)\n",
        "if faceshq:\n",
        "    downloded_model = download_vqgan_model(\n",
        "        name='faceshq',\n",
        "        config_url='https://drive.google.com/uc?export=download&id=1fHwGx_hnBtC8nsq7hesJvs-Klv-P0gzT',\n",
        "        checkpoint_url='https://app.koofr.net/content/links/a04deec9-0c59-4673-8b37-3d696fe63a5d/files/get/last.ckpt?path=%2F2020-11-13T21-41-45_faceshq_transformer%2Fcheckpoints%2Flast.ckpt')\n",
        "    models.append(downloded_model)\n",
        "if wikiart_16384:\n",
        "    downloded_model = download_vqgan_model(\n",
        "        name='wikiart_16384',\n",
        "        config_url='http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.yaml',\n",
        "        checkpoint_url='http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.ckpt')\n",
        "    models.append(downloded_model)\n",
        "if sflckr:\n",
        "    downloded_model = download_vqgan_model(\n",
        "        name='sflckr',\n",
        "        config_url='https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fconfigs%2F2020-11-09T13-31-51-project.yaml&dl=1',\n",
        "        checkpoint_url='https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fcheckpoints%2Flast.ckpt&dl=1')\n",
        "    models.append(downloded_model)\n",
        "\n",
        "print(models)\n",
        "\n",
        "# these are so small we don't bother caching them\n",
        "srcnn_models_url = 'https://raw.githubusercontent.com/justinjohn0306/SRCNN/master/models/'\n",
        "for srcnn_model in [\"2x\", \"3x\", \"4x\"]:\n",
        "    urlretrieve(f\"{srcnn_models_url}model_{srcnn_model}.pth\", join(models_dir, f\"model_{srcnn_model}.pth\"))\n",
        "\n",
        "from os.path import exists\n",
        "def download_from_google_drive(file_id, file_name):\n",
        "  # download a file from the Google Drive link\n",
        "  # TODO: convert this to pure Python\n",
        "  !rm -f ./cookie\n",
        "  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id={file_id}\" > /dev/null\n",
        "  confirm_text = !awk '/download/ {print $NF}' ./cookie\n",
        "  confirm_text = confirm_text[0]\n",
        "  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm={confirm_text}&id={file_id}\" -o {file_name}\n",
        "  \n",
        "pretrained_model = 'SuperSloMo.ckpt'\n",
        "if not exists(pretrained_model):\n",
        "  download_from_google_drive('1IvobLDbRiBgZr3ryCRrWL8xDbMZ-KnpF', pretrained_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rXx8O5Fvgm6v"
      },
      "outputs": [],
      "source": [
        "# @title ## Loading of libraries and definitions\n",
        " \n",
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import subprocess\n",
        "from shutil import copy\n",
        "from os.path import join\n",
        " \n",
        "sys.path.append('./taming-transformers')\n",
        "\n",
        "# Some models include transformers, others need explicit pip install\n",
        "try:\n",
        "    import transformers\n",
        "except Exception:\n",
        "    !pip install transformers\n",
        "    import transformers\n",
        "\n",
        "from IPython import display\n",
        "from base64 import b64encode\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        " \n",
        "from CLIP import clip\n",
        "import kornia.augmentation as K\n",
        "import numpy as np\n",
        "import imageio\n",
        "from PIL import ImageFile, Image\n",
        "from imgtag import ImgTag    # metadata \n",
        "from libxmp import *         # metadata\n",
        "import libxmp                # metadata\n",
        "from stegano import lsb\n",
        "import json\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        " \n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        " \n",
        " \n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        " \n",
        " \n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        " \n",
        " \n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        " \n",
        "    input = input.view([n * c, 1, h, w])\n",
        " \n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        " \n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        " \n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        " \n",
        " \n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        " \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        " \n",
        " \n",
        "replace_grad = ReplaceGrad.apply\n",
        " \n",
        " \n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        " \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        " \n",
        " \n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        " \n",
        " \n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        " \n",
        " \n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        " \n",
        "    def forward(self, input):\n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        " \n",
        " \n",
        "def parse_prompt(prompt):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        " \n",
        " \n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.augs = nn.Sequential(\n",
        "            K.RandomHorizontalFlip(p=0.5),\n",
        "            # K.RandomSolarize(0.01, 0.01, p=0.7),\n",
        "            K.RandomSharpness(0.3,p=0.4),\n",
        "            K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n",
        "            K.RandomPerspective(0.2,p=0.4),\n",
        "            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7))\n",
        "        self.noise_fac = 0.1\n",
        " \n",
        " \n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
        "        if self.noise_fac:\n",
        "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "            batch = batch + facs * torch.randn_like(batch)\n",
        "        return batch\n",
        " \n",
        " \n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        " \n",
        " \n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9SqvStcBgr_0"
      },
      "outputs": [],
      "source": [
        "#@title ## Configure Arguments for the Neural Network\n",
        "if initial_image != \"\":\n",
        "    print(\n",
        "        \"WARNING: You have specified an initial image. Note that the image resolution \"\n",
        "        \"will be inherited from this image, not whatever width and height you specified. \"\n",
        "        \"If the initial image resolution is too high, this can result in out of memory errors.\"\n",
        "    )\n",
        "elif width * height > 160000:\n",
        "    print(\n",
        "        \"WARNING: The width and height you have specified may be too high, in which case \"\n",
        "        \"you will encounter out of memory errors either at the image generation stage or the \"\n",
        "        \"video synthesis stage. If so, try reducing the resolution\"\n",
        "    )\n",
        "\n",
        "if seed == -1:\n",
        "    seed = None\n",
        "\n",
        "def parse_key_frames(string, prompt_parser=None):\n",
        "    import re\n",
        "    pattern = r'((?P<frame>[0-9]+):[\\s]*[\\(](?P<param>[\\S\\s]*?)[\\)])'\n",
        "    frames = dict()\n",
        "    for match_object in re.finditer(pattern, string):\n",
        "        frame = int(match_object.groupdict()['frame'])\n",
        "        param = match_object.groupdict()['param']\n",
        "        if prompt_parser:\n",
        "            frames[frame] = prompt_parser(param)\n",
        "        else:\n",
        "            frames[frame] = param\n",
        "\n",
        "    if frames == {} and len(string) != 0:\n",
        "        raise RuntimeError('Key Frame string not correctly formatted')\n",
        "    return frames\n",
        "\n",
        "def get_inbetweens(key_frames, integer=False):\n",
        "    key_frame_series = pd.Series([np.nan for a in range(max_frames)])\n",
        "    for i, value in key_frames.items():\n",
        "        key_frame_series[i] = value\n",
        "    key_frame_series = key_frame_series.astype(float)\n",
        "    key_frame_series = key_frame_series.interpolate(limit_direction='both')\n",
        "    if integer:\n",
        "        return key_frame_series.astype(int)\n",
        "    return key_frame_series\n",
        "\n",
        "def split_key_frame_text_prompts(frames):\n",
        "    prompt_dict = dict()\n",
        "    for i, parameters in frames.items():\n",
        "        prompts = parameters.split('|')\n",
        "        for prompt in prompts:\n",
        "            string, value = prompt.split(':')\n",
        "            string = string.strip()\n",
        "            value = float(value.strip())\n",
        "            if string in prompt_dict:\n",
        "                prompt_dict[string][i] = value\n",
        "            else:\n",
        "                prompt_dict[string] = {i: value}\n",
        "    prompt_series_dict = dict()\n",
        "    for prompt, values in prompt_dict.items():\n",
        "        value_string = (\n",
        "            ', '.join([f'{value}: ({values[value]})' for value in values])\n",
        "        )\n",
        "        prompt_series = get_inbetweens(parse_key_frames(value_string))\n",
        "        prompt_series_dict[prompt] = prompt_series\n",
        "    prompt_list = []\n",
        "    for i in range(max_frames):\n",
        "        prompt_list.append(\n",
        "            ' | '.join(\n",
        "                [f'{prompt}: {prompt_series_dict[prompt][i]}'\n",
        "                 for prompt in prompt_series_dict]\n",
        "            )\n",
        "        )\n",
        "    return prompt_list\n",
        "\n",
        "if key_frames:\n",
        "    try:\n",
        "        text_prompts_series = split_key_frame_text_prompts(\n",
        "            parse_key_frames(text_prompts)\n",
        "        )\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `text_prompts` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `text_prompts` as \"\n",
        "            f'\"0: ({text_prompts}:1)\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        text_prompts = f\"0: ({text_prompts}:1)\"\n",
        "        text_prompts_series = split_key_frame_text_prompts(\n",
        "            parse_key_frames(text_prompts)\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        target_images_series = split_key_frame_text_prompts(\n",
        "            parse_key_frames(target_images)\n",
        "        )\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `target_images` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `target_images` as \"\n",
        "            f'\"0: ({target_images}:1)\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        target_images = f\"0: ({target_images}:1)\"\n",
        "        target_images_series = split_key_frame_text_prompts(\n",
        "            parse_key_frames(target_images)\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        angle_series = get_inbetweens(parse_key_frames(angle))\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `angle` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `angle` as \"\n",
        "            f'\"0: ({angle})\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        angle = f\"0: ({angle})\"\n",
        "        angle_series = get_inbetweens(parse_key_frames(angle))\n",
        "\n",
        "    try:\n",
        "        zoom_series = get_inbetweens(parse_key_frames(zoom))\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `zoom` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `zoom` as \"\n",
        "            f'\"0: ({zoom})\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        zoom = f\"0: ({zoom})\"\n",
        "        zoom_series = get_inbetweens(parse_key_frames(zoom))\n",
        "\n",
        "    try:\n",
        "        translation_x_series = get_inbetweens(parse_key_frames(translation_x))\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `translation_x` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `translation_x` as \"\n",
        "            f'\"0: ({translation_x})\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        translation_x = f\"0: ({translation_x})\"\n",
        "        translation_x_series = get_inbetweens(parse_key_frames(translation_x))\n",
        "\n",
        "    try:\n",
        "        translation_y_series = get_inbetweens(parse_key_frames(translation_y))\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `translation_y` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `translation_y` as \"\n",
        "            f'\"0: ({translation_y})\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        translation_y = f\"0: ({translation_y})\"\n",
        "        translation_y_series = get_inbetweens(parse_key_frames(translation_y))\n",
        "\n",
        "    try:\n",
        "        iterations_per_frame_series = get_inbetweens(\n",
        "            parse_key_frames(iterations_per_frame), integer=True\n",
        "        )\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `iterations_per_frame` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `iterations_per_frame` as \"\n",
        "            f'\"0: ({iterations_per_frame})\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        iterations_per_frame = f\"0: ({iterations_per_frame})\"\n",
        "        \n",
        "        iterations_per_frame_series = get_inbetweens(\n",
        "            parse_key_frames(iterations_per_frame), integer=True\n",
        "        )\n",
        "else:\n",
        "    text_prompts = [phrase.strip() for phrase in text_prompts.split(\"|\")]\n",
        "    if text_prompts == ['']:\n",
        "        text_prompts = []\n",
        "    if target_images == \"None\" or not target_images:\n",
        "        target_images = []\n",
        "    else:\n",
        "        target_images = target_images.split(\"|\")\n",
        "        target_images = [image.strip() for image in target_images]\n",
        "\n",
        "    angle = float(angle)\n",
        "    zoom = float(zoom)\n",
        "    translation_x = float(translation_x)\n",
        "    translation_y = float(translation_y)\n",
        "    iterations_per_frame = int(iterations_per_frame)\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    prompts=text_prompts,\n",
        "    image_prompts=target_images,\n",
        "    noise_prompt_seeds=[],\n",
        "    noise_prompt_weights=[],\n",
        "    size=[width, height],\n",
        "    init_weight=0.,\n",
        "    clip_model='ViT-B/32',\n",
        "    vqgan_config=join(models_dir, f'{vqgan_model}.yaml'),\n",
        "    vqgan_checkpoint=join(models_dir, f'{vqgan_model}.ckpt'),\n",
        "    step_size=0.1,\n",
        "    cutn=64,\n",
        "    cut_pow=1.,\n",
        "    display_freq=interval,\n",
        "    seed=seed,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title ## Video Creation Helpers\n",
        "#@markdown `ffmpeg` will need to be installed\n",
        "\n",
        "import ffmpeg\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "\n",
        "def display_video_in_output(video_file):\n",
        "    video_data = open(video_file,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(video_data).decode()\n",
        "    display.HTML(f'<video width=400 controls><source src=\"{data_url}\" type=\"video/mp4\"></video>')\n",
        "\n",
        "def create_video_from_dir(directory, video_file, input_height, input_width, framerate):\n",
        "    if isfile(video_file):\n",
        "        os.remove(video_file)\n",
        "\n",
        "    print(f'creating {video_file} from {directory} as a {framerate}fps {input_height}x{input_width} mp4.')\n",
        "\n",
        "    (\n",
        "    ffmpeg\n",
        "        .input(f'{directory}/%04d.png', pattern_type='sequence', s=f'{input_height}x{input_width}', framerate=framerate)\n",
        "        .output(video_file, preset='fast')\n",
        "        .run()\n",
        "    )\n",
        "    display_video_in_output(video_file)\n",
        "\n",
        "latest_video = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image Synthesis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "owUFbInCHW9W"
      },
      "outputs": [],
      "source": [
        "#@title ## Fire up the Neural Network\n",
        "\n",
        "# Delete memory from previous runs\n",
        "!nvidia-smi -caa\n",
        "for var in ['device', 'model', 'perceptor', 'z']:\n",
        "    try:\n",
        "        del globals()[var]\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "try:\n",
        "    import gc\n",
        "    gc.collect()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    torch.cuda.empty_cache()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "if not key_frames:\n",
        "    if text_prompts:\n",
        "        print('Using text prompts:', text_prompts)\n",
        "    if target_images:\n",
        "        print('Using image prompts:', target_images)\n",
        "\n",
        "if args.seed is None:\n",
        "    seed = torch.seed()\n",
        "else:\n",
        "    seed = args.seed\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "print('Using seed:', seed)\n",
        "\n",
        "# copy the initial image into the steps folder\n",
        "if (init_frame > 1) and (len(initial_image) > 0):\n",
        "    copy(initial_image, join(working_dir, 'steps', os.path.split(initial_image)[1]))\n",
        " \n",
        "model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        " \n",
        "cut_size = perceptor.visual.input_resolution\n",
        "e_dim = model.quantize.e_dim\n",
        "f = 2**(model.decoder.num_resolutions - 1)\n",
        "make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "n_toks = model.quantize.n_e\n",
        "toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "sideX, sideY = toksX * f, toksY * f\n",
        "z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "stop_on_next_loop = False  # Make sure GPU memory doesn't get corrupted from cancelling the run mid-way through, allow a full frame to complete\n",
        "\n",
        "def read_image_workaround(path):\n",
        "    \"\"\"OpenCV reads images as BGR, Pillow saves them as RGB. Work around\n",
        "    this incompatibility to avoid colour inversions.\"\"\"\n",
        "    im_tmp = cv2.imread(path)\n",
        "    return cv2.cvtColor(im_tmp, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "for i in range(init_frame, max_frames):\n",
        "    if stop_on_next_loop:\n",
        "      break\n",
        "    if key_frames:\n",
        "        text_prompts = text_prompts_series[i]\n",
        "        text_prompts = [phrase.strip() for phrase in text_prompts.split(\"|\")]\n",
        "        if text_prompts == ['']:\n",
        "            text_prompts = []\n",
        "        args.prompts = text_prompts\n",
        "\n",
        "        target_images = target_images_series[i]\n",
        "\n",
        "        if target_images == \"None\" or not target_images:\n",
        "            target_images = []\n",
        "        else:\n",
        "            target_images = target_images.split(\"|\")\n",
        "            target_images = [image.strip() for image in target_images]\n",
        "        args.image_prompts = target_images\n",
        "\n",
        "        angle = angle_series[i]\n",
        "        zoom = zoom_series[i]\n",
        "        translation_x = translation_x_series[i]\n",
        "        translation_y = translation_y_series[i]\n",
        "        iterations_per_frame = iterations_per_frame_series[i]\n",
        "        print(\n",
        "            f'text_prompts: {text_prompts}'\n",
        "            f'angle: {angle}',\n",
        "            f'zoom: {zoom}',\n",
        "            f'translation_x: {translation_x}',\n",
        "            f'translation_y: {translation_y}',\n",
        "            f'iterations_per_frame: {iterations_per_frame}'\n",
        "        )\n",
        "    try:\n",
        "        if i == 0 and len(initial_image.strip()) > 0:\n",
        "            img_0 = read_image_workaround(initial_image)\n",
        "            z, *_ = model.encode(TF.to_tensor(img_0).to(device).unsqueeze(0) * 2 - 1)\n",
        "        elif i == 0 and not os.path.isfile(f'{working_dir}/steps/{i:04d}.png'):\n",
        "            one_hot = F.one_hot(\n",
        "                torch.randint(n_toks, [toksY * toksX], device=device), n_toks\n",
        "            ).float()\n",
        "            z = one_hot @ model.quantize.embedding.weight\n",
        "            z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "        else:\n",
        "            if save_all_iterations:\n",
        "                img_0 = read_image_workaround(\n",
        "                    f'{working_dir}/steps/{i:04d}_{iterations_per_frame}.png')\n",
        "            else:\n",
        "                img_0 = read_image_workaround(f'{working_dir}/steps/{i:04d}.png')\n",
        "\n",
        "            center = (1*img_0.shape[1]//2, 1*img_0.shape[0]//2)\n",
        "            trans_mat = np.float32(\n",
        "                [[1, 0, translation_x],\n",
        "                [0, 1, translation_y]]\n",
        "            )\n",
        "            rot_mat = cv2.getRotationMatrix2D( center, angle, zoom )\n",
        "\n",
        "            trans_mat = np.vstack([trans_mat, [0,0,1]])\n",
        "            rot_mat = np.vstack([rot_mat, [0,0,1]])\n",
        "            transformation_matrix = np.matmul(rot_mat, trans_mat)\n",
        "\n",
        "            img_0 = cv2.warpPerspective(\n",
        "                img_0,\n",
        "                transformation_matrix,\n",
        "                (img_0.shape[1], img_0.shape[0]),\n",
        "                borderMode=cv2.BORDER_WRAP\n",
        "            )\n",
        "            z, *_ = model.encode(TF.to_tensor(img_0).to(device).unsqueeze(0) * 2 - 1)\n",
        "        i += 1\n",
        "\n",
        "        z_orig = z.clone()\n",
        "        z.requires_grad_(True)\n",
        "        opt = optim.Adam([z], lr=args.step_size)\n",
        "\n",
        "        normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                        std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "        pMs = []\n",
        "\n",
        "        for prompt in args.prompts:\n",
        "            txt, weight, stop = parse_prompt(prompt)\n",
        "            embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "            pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "        for prompt in args.image_prompts:\n",
        "            path, weight, stop = parse_prompt(prompt)\n",
        "            img = resize_image(Image.open(path).convert('RGB'), (sideX, sideY))\n",
        "            batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "            embed = perceptor.encode_image(normalize(batch)).float()\n",
        "            pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "        for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "            gen = torch.Generator().manual_seed(seed)\n",
        "            embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "            pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "        def synth(z):\n",
        "            z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "            return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "        def add_xmp_data(filename):\n",
        "            imagen = ImgTag(filename=filename)\n",
        "            imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'creator', 'VQGAN+CLIP', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "            if args.prompts:\n",
        "                imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', \" | \".join(args.prompts), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "            else:\n",
        "                imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', 'None', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "            imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'i', str(i), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "            imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'model', model_name, {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "            imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'seed',str(seed) , {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "            imagen.close()\n",
        "\n",
        "        def add_stegano_data(filename):\n",
        "            data = {\n",
        "                \"title\": \" | \".join(args.prompts) if args.prompts else None,\n",
        "                \"notebook\": \"VQGAN+CLIP\",\n",
        "                \"i\": i,\n",
        "                \"model\": model_name,\n",
        "                \"seed\": str(seed),\n",
        "            }\n",
        "            lsb.hide(filename, json.dumps(data)).save(filename)\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def checkin(i, losses):\n",
        "            losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "            tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "            out = synth(z)\n",
        "            TF.to_pil_image(out[0].cpu()).save('progress.png')\n",
        "            add_stegano_data('progress.png')\n",
        "            add_xmp_data('progress.png')\n",
        "            display.display(display.Image('progress.png'))\n",
        "\n",
        "        def save_output(i, img, suffix=None):\n",
        "            filename = f\"{i:04}{'_' + suffix if suffix else ''}.png\"\n",
        "            steps_dir = join(working_dir, 'steps')\n",
        "            out_file = join(steps_dir, filename)\n",
        "            imageio.imwrite(out_file, np.array(img))\n",
        "            add_stegano_data(out_file)\n",
        "            add_xmp_data(out_file)\n",
        "            project.add_artifact('steps', i, out_file)\n",
        "\n",
        "        def ascend_txt(i, save=True, suffix=None):\n",
        "            out = synth(z)\n",
        "            iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n",
        "\n",
        "            result = []\n",
        "\n",
        "            if args.init_weight:\n",
        "                result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n",
        "\n",
        "            for prompt in pMs:\n",
        "                result.append(prompt(iii))\n",
        "            img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n",
        "            img = np.transpose(img, (1, 2, 0))\n",
        "            if save:\n",
        "                save_output(i, img, suffix=suffix)\n",
        "            return result\n",
        "\n",
        "        def train(i, save=True, suffix=None):\n",
        "            opt.zero_grad()\n",
        "            lossAll = ascend_txt(i, save=save, suffix=suffix)\n",
        "            if i % args.display_freq == 0 and save:\n",
        "                checkin(i, lossAll)\n",
        "            loss = sum(lossAll)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            with torch.no_grad():\n",
        "                z.copy_(z.maximum(z_min).minimum(z_max))\n",
        "\n",
        "        with tqdm() as pbar:\n",
        "            if iterations_per_frame == 0:\n",
        "                save_output(i, img_0)\n",
        "            j = 1\n",
        "            while True:\n",
        "                suffix = (str(j) if save_all_iterations else None)\n",
        "                if j >= iterations_per_frame:\n",
        "                    train(i, save=True, suffix=suffix)\n",
        "                    break\n",
        "                if save_all_iterations:\n",
        "                    train(i, save=True, suffix=suffix)\n",
        "                else:\n",
        "                    train(i, save=False, suffix=suffix)\n",
        "                j += 1\n",
        "                pbar.update()\n",
        "    except KeyboardInterrupt:\n",
        "      stop_on_next_loop = True\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create preview video from generated frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "if generate_videos:\n",
        "    steps_frames_dir = join(working_dir, 'steps')\n",
        "    steps_video = join(working_dir, 'steps.mp4')\n",
        "\n",
        "    create_video_from_dir(steps_frames_dir, steps_video, height, width, 12)\n",
        "\n",
        "    project.add_output('steps', steps_video)\n",
        "    latest_video = steps_video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Crop Frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from os import listdir, remove\n",
        "from os.path import isfile, join\n",
        "from pathlib import Path\n",
        "\n",
        "if crop_frames:\n",
        "    steps_dir = join(working_dir, \"steps\")\n",
        "    cropped_steps_dir = join(working_dir, \"cropped_steps\")\n",
        "    Path(cropped_steps_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for source_frame_filename in sorted(listdir(steps_dir)):\n",
        "        step_filename = join(steps_dir, source_frame_filename)\n",
        "        cropped_filename = join(cropped_steps_dir, source_frame_filename)\n",
        "        frame_number = int(source_frame_filename.split('.')[0])\n",
        "\n",
        "        print(f'Cropping frame {frame_number} ({step_filename} => {cropped_filename}).')\n",
        "\n",
        "        if isfile(cropped_filename):\n",
        "            remove(cropped_filename)\n",
        "\n",
        "        original = Image.open(step_filename)\n",
        "        width, height = original.size\n",
        "\n",
        "        left = trim_width / 2\n",
        "        top = trim_height / 2\n",
        "        right = width - (trim_width / 2)\n",
        "        bottom = height - (trim_height / 2)\n",
        "\n",
        "        cropped_frame = original.crop((left, top, right, bottom))\n",
        "\n",
        "        cropped_frame.save(cropped_filename)\n",
        "        project.add_artifact(\"cropped_steps\", frame_number, cropped_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create video from cropped frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if generate_videos:\n",
        "    cropped_frames_dir = join(working_dir, 'cropped_steps')\n",
        "    cropped_video = join(working_dir, 'cropped_steps.mp4')\n",
        "\n",
        "    create_video_from_dir(cropped_frames_dir, cropped_video, height - trim_height, width - trim_width, 12)\n",
        "\n",
        "    project.add_output('cropped_steps', cropped_video)\n",
        "    latest_video = cropped_video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Increase Resolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "lp0_TdGHHjtM"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import shutil\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from pathlib import Path\n",
        "\n",
        "if upscale_frames:\n",
        "    zoom_factor = superscale_resolution.rstrip(\"x\")\n",
        "\n",
        "    cropped_steps_dir = join(working_dir, \"cropped_steps\")\n",
        "    zoomed_steps_dir = join(working_dir, \"zoomed_steps\")\n",
        "    Path(zoomed_steps_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for cropped_frame in sorted(listdir(cropped_steps_dir)):\n",
        "        cropped_frame_filename = join(cropped_steps_dir, cropped_frame) \n",
        "        zoomed_frame_filename = join(zoomed_steps_dir, cropped_frame)\n",
        "        frame_number = int(cropped_frame.split('.')[0])\n",
        "\n",
        "        cmd = [\n",
        "            'python3',\n",
        "            f'{working_dir}/SRCNN/run.py',\n",
        "            '--zoom_factor',\n",
        "            zoom_factor,\n",
        "            '--model',\n",
        "            f\"{working_dir}/models/model_{superscale_resolution}.pth\",  # 2x, 3x and 4x are available from the repo above\n",
        "            '--image',\n",
        "            cropped_frame,\n",
        "            '--cuda'\n",
        "        ]\n",
        "        print(f'Upscaling frame {frame_number} ({cropped_frame_filename})')\n",
        "\n",
        "        if isfile(zoomed_frame_filename):\n",
        "            remove(zoomed_frame_filename)\n",
        "\n",
        "        process = subprocess.Popen(cmd, cwd=cropped_steps_dir)\n",
        "        stdout, stderr = process.communicate()\n",
        "        if process.returncode != 0:\n",
        "            print(stdout)\n",
        "            print(stderr)\n",
        "            raise RuntimeError(stderr)\n",
        "\n",
        "        shutil.move(join(cropped_steps_dir, f\"zoomed_{cropped_frame}\"), zoomed_frame_filename)\n",
        "        project.add_artifact(\"zoomed\", frame_number, zoomed_frame_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Crate video from zoomed frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if generate_videos:\n",
        "    zoomed_frames_dir = join(working_dir, 'zoomed_steps')\n",
        "    zoomed_video = join(working_dir, 'zoomed_steps.mp4')\n",
        "\n",
        "    zoom_factor = superscale_resolution.rstrip(\"x\")\n",
        "\n",
        "    create_video_from_dir(zoomed_frames_dir, zoomed_video, height * int(zoom_factor), width * int(zoom_factor), 12)\n",
        "\n",
        "    project.add_output('zoomed_steps', zoomed_video)\n",
        "    latest_video = zoomed_video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UvfjxCGHwz2"
      },
      "source": [
        "# Super-Slomo for smoothing movement\n",
        "\n",
        "This step might run out of memory if you run it right after the steps above. If it does, restart the notebook, upload a saved copy of the video from the previous step (or get it from google drive) and define the variable `filepath` with the path to the video before running the cells below again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ybgyF2flcmME"
      },
      "outputs": [],
      "source": [
        "# import subprocess in case this cell is run without the above cells\n",
        "import subprocess\n",
        "\n",
        "if super_slomo:\n",
        "    SLOW_MOTION_FACTOR = 2 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "    TARGET_FPS = 14 #@param {type:\"slider\", min:0, max:30, step:1}\n",
        "\n",
        "    latest_video_stem = latest_video.split('.')[0]\n",
        "\n",
        "    cmd1 = [\n",
        "        'python',\n",
        "        'Super-SloMo/video_to_slomo.py',\n",
        "        '--checkpoint',\n",
        "        pretrained_model,\n",
        "        '--video',\n",
        "        latest_video,\n",
        "        '--sf',\n",
        "        str(SLOW_MOTION_FACTOR),\n",
        "        '--fps',\n",
        "        str(TARGET_FPS),\n",
        "        '--output',\n",
        "        f'{latest_video_stem}-slomo.mkv',\n",
        "    ]\n",
        "\n",
        "    process = subprocess.Popen(cmd1, cwd=f'/content', stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)\n",
        "    stdout, stderr = process.communicate()\n",
        "    if process.returncode != 0:\n",
        "        raise RuntimeError(stderr)\n",
        "\n",
        "    cmd2 = [\n",
        "        'ffmpeg',\n",
        "        '-i',\n",
        "        f'{latest_video_stem}-slomo.mkv',\n",
        "        '-pix_fmt',\n",
        "        'yuv420p',\n",
        "        '-crf',\n",
        "        '17',\n",
        "        '-preset',\n",
        "        'veryslow',\n",
        "        f'{latest_video_stem}-slomo.mp4',\n",
        "    ]\n",
        "\n",
        "    process = subprocess.Popen(cmd2, cwd=f'/content', stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    stdout, stderr = process.communicate()\n",
        "    if process.returncode != 0:\n",
        "        print(stderr)\n",
        "        print(\n",
        "            \"You may be able to avoid this error by backing up the frames,\"\n",
        "            \"restarting the notebook, and running only the video synthesis cells,\"\n",
        "            \"or by decreasing the resolution of the image generation steps. \"\n",
        "            \"If you restart the notebook, you will have to define the `filepath` manually\"\n",
        "            \"by adding `filepath = 'PATH_TO_THE_VIDEO'` to the beginning of this cell. \"\n",
        "            \"If these steps do not work, please post the traceback in the github.\"\n",
        "        )\n",
        "        raise RuntimeError(stderr)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "ai-animating-intermediate.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
