{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing VQGAN Images\n",
    "\n",
    "## Check for missing images in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"../content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in c:\\users\\mcp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (9.0.1)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install --upgrade Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating frames:\n",
      "All frames appear to be in place.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "print(\"Enumerating frames:\")\n",
    "steps_dir = join(working_dir, \"steps\")\n",
    "files = [f for f in listdir(steps_dir) if isfile(join(steps_dir, f))]\n",
    "dirty = False\n",
    "\n",
    "for i in range(1, len(files) + 1):\n",
    "    frame = join(steps_dir, f\"{i:04d}.png\")\n",
    "    if (not isfile(frame)):\n",
    "        print(f\"  {frame} missing\")\n",
    "        dirty = True\n",
    "        continue\n",
    "    original = Image.open(frame)\n",
    "    width, height = original.size\n",
    "    if width != 368:\n",
    "        print(f\"  {frame} width is {width} rather than 368px\")\n",
    "        dirty = True\n",
    "    if height != 640:\n",
    "        print(f\"  {frame} height is {height} rather than 368px\")\n",
    "        dirty = True\n",
    "\n",
    "if dirty:\n",
    "    print(\"At least one item in the sequence is missing or incorrect!\")\n",
    "else:\n",
    "    print(f\"All frames appear to be in place.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cropping the image\n",
    "\n",
    "Due to the way the height and width are cacluated in Katherine Crowson's python code you can lose 8 pixels from 360px, we can compensate for this by rendering the images at 368px however this means we have 4px on either side that is vestigial for a 9:16 resolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path\n",
    "\n",
    "steps_dir = join(working_dir, \"steps\")\n",
    "cropped_steps_dir = join(working_dir, \"cropped_steps\")\n",
    "Path(cropped_steps_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for i in range(1, len(files) + 1):\n",
    "    frame_filename = f\"{i:04d}.png\"\n",
    "    frame = join(steps_dir, frame_filename)\n",
    "    target_frame_file = join(cropped_steps_dir, frame_filename)\n",
    "\n",
    "    if os.path.isfile(target_frame_file):\n",
    "        continue\n",
    "\n",
    "    original = Image.open(frame)\n",
    "    width, height = original.size\n",
    "\n",
    "    targetWidth, targetHeight = 360, 640\n",
    "    \n",
    "    left = (width - targetWidth) / 2\n",
    "    top = (height - targetHeight) / 2\n",
    "    right = width - ((width - targetWidth) / 2)\n",
    "    bottom = height - ((height - targetHeight) / 2)\n",
    "\n",
    "    cropped_frame = original.crop((left, top, right, bottom))\n",
    "\n",
    "    cropped_frame.save(target_frame_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Sampling with SRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_dir = join(working_dir, \"ais\")\n",
    "srcnn_dir = join(ais_dir, \"SRCNN\")\n",
    "\n",
    "!git clone https://github.com/Mirwaisse/SRCNN.git $srcnn_dir\n",
    "\n",
    "!python3 -m pip install --upgrade numpy \n",
    "!python3 -m pip install --upgrade Pillow \n",
    "!python3 -m pip install --pre torch torchvision torchaudio -f https://download.pytorch.org/whl/nightly/cu113/torch_nightly.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "models_dir = join(working_dir, \"models\")\n",
    "Path(models_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for model in [\"2x\", \"3x\", \"4x\"]:\n",
    "    urllib.request.urlretrieve(f\"https://raw.githubusercontent.com/justinjohn0306/SRCNN/master/models/model_{model}.pth\", join(models_dir, f\"model_{model}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upscaling frame 136 (0136.png)\n",
      "Upscaling frame 137 (0137.png)\n",
      "Upscaling frame 138 (0138.png)\n",
      "Upscaling frame 139 (0139.png)\n",
      "Upscaling frame 140 (0140.png)\n",
      "Upscaling frame 141 (0141.png)\n",
      "Upscaling frame 142 (0142.png)\n",
      "Upscaling frame 143 (0143.png)\n",
      "Upscaling frame 144 (0144.png)\n",
      "Upscaling frame 145 (0145.png)\n",
      "Upscaling frame 146 (0146.png)\n",
      "Upscaling frame 147 (0147.png)\n",
      "Upscaling frame 148 (0148.png)\n",
      "Upscaling frame 149 (0149.png)\n",
      "Upscaling frame 150 (0150.png)\n",
      "Upscaling frame 151 (0151.png)\n",
      "Upscaling frame 152 (0152.png)\n",
      "Upscaling frame 153 (0153.png)\n",
      "Upscaling frame 154 (0154.png)\n",
      "Upscaling frame 155 (0155.png)\n",
      "Upscaling frame 156 (0156.png)\n",
      "Upscaling frame 157 (0157.png)\n",
      "Upscaling frame 158 (0158.png)\n",
      "Upscaling frame 159 (0159.png)\n",
      "Upscaling frame 160 (0160.png)\n",
      "Upscaling frame 161 (0161.png)\n",
      "Upscaling frame 162 (0162.png)\n",
      "Upscaling frame 163 (0163.png)\n",
      "Upscaling frame 164 (0164.png)\n",
      "Upscaling frame 165 (0165.png)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path\n",
    "\n",
    "# Set zoomed = True if this cell is run\n",
    "zoomed = True\n",
    "\n",
    "resolution = \"3x\" #@param [\"2x\", \"3x\", \"4x\"] {type:\"string\"}\n",
    "zoom_factor = resolution.rstrip(\"x\")\n",
    "\n",
    "cropped_steps_dir = join(working_dir, \"cropped_steps\")\n",
    "zoomed_steps_dir = join(working_dir, \"zoomed_steps\")\n",
    "Path(zoomed_steps_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for i in range(1, len(files) + 1):\n",
    "    frame_filename = f\"{i:04d}.png\"\n",
    "    target_frame_file = join(zoomed_steps_dir, frame_filename)\n",
    "\n",
    "    if isfile(target_frame_file):\n",
    "        continue\n",
    "\n",
    "    cmd = [\n",
    "        'python3',\n",
    "        '../../content/ais/SRCNN/run.py',\n",
    "        '--zoom_factor',\n",
    "        zoom_factor,  # Note if you increase this, you also need to change the model.\n",
    "        '--model',\n",
    "        f\"../../content/models/model_{resolution}.pth\",  # 2x, 3x and 4x are available from the repo above\n",
    "        '--image',\n",
    "        frame_filename,\n",
    "        '--cuda'\n",
    "    ]\n",
    "    print(f'Upscaling frame {i} ({frame_filename})')\n",
    "\n",
    "    process = subprocess.Popen(cmd, cwd=cropped_steps_dir)\n",
    "    stdout, stderr = process.communicate()\n",
    "    if process.returncode != 0:\n",
    "        print(stdout)\n",
    "        print(stderr)\n",
    "        raise RuntimeError(stderr)\n",
    "\n",
    "    shutil.move(join(cropped_steps_dir, f\"zoomed_{frame_filename}\"), target_frame_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg-python in c:\\users\\mcp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.2.0)\n",
      "Requirement already satisfied: future in c:\\users\\mcp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from ffmpeg-python) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ffmpeg\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "\n",
    "zoomed_steps_dir = os.path.abspath(join(working_dir, \"zoomed_steps\"))\n",
    "video_file = os.path.abspath(join(working_dir, \"video.mp4\"))\n",
    "\n",
    "if isfile(video_file):\n",
    "    os.remove(video_file)\n",
    "\n",
    "(\n",
    "ffmpeg\n",
    "    .input(f'{zoomed_steps_dir}\\%04d.png', pattern_type='sequence', s='1080x1920', framerate=12)\n",
    "    .output(video_file, preset='fast')\n",
    "    .run()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76e39a95be4d5898a3570c447c27edaa62298e3a74d4da47c7cca3211a7c59ee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
